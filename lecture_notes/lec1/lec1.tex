\documentclass[10pt]{article}

\usepackage{parskip}
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb, graphicx, multicol, array}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{href-ul}
 
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
 
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\begin{document}
 
\title{Lecture 1}
\author{Jakob Sverre Alexandersen\\
GRA4153 Advanced Statistics}
\maketitle

\tableofcontents
\newpage
 
\section{Introduction}

Finna learn which analyses to undertake and how to interpret them\\

\textbf{Divided into 4 parts:}

\begin{enumerate}
    \item Basic probability theory
    \item Statistical inference
    \item Linear (regression) models
    \item Time series analysis
\end{enumerate}

Lecture notes are the primary course material

Exercises are heavily useful $\to$ do these every week

\subsection{Asessment}

\begin{itemize}
    \item Part 1 – 40\% – midterm (in groups of up to 3)
    \item Part 2 – 60\% – final take-home exams
\end{itemize}

\subsection{Contact}
\begin{itemize}
    \item Office Hours: by apt
    \item Contact: \href{mailto:adam.lee@bi.no}{Adam Lee}
    \item for god's sake use email
\end{itemize}

\newpage

\section{Probability}

\begin{align*}
    S: \text{Sample space} \to \{x_1, …, x_n\} \to \text{cannot have duplicate numbers}\\
    \mathcal{S}: \sigma\text{-algebra }\text{set of sets}\\
    S = \{1, 2, 3\}\\
    A \subset S\\
    A = \{1, 2\} \quad A = \{1\} \quad A = \{\} \to \text{This is the empty set $ø$}\\
    P: \mathcal{S} \to \mathbb{R} \text{ is a probability IF} \\
    i) P(A) \geq 0 \quad\forall A \in \mathcal{S}\\
    ii) P(\mathcal{S}) = 1\\
    iii) A_1, …, A_n \text{ s.t. } i \neq j =) A_i \cap A_j = ø \to P(u_{1, 2}^{\infty}) = \sum_{i = 1}^{\infty}P(A_i)
\end{align*}


\subsection{2.1.1 Events and probabilities}

\begin{align*}
    \text{Prop: if } P : \mathcal{S} \to \mathbb{R} \text{ is a probability} \text{ s.t. } A \in \mathcal{S}\\
    \text{then: }\\
    i) P(ø) = 0\\
    \mathcal{S}\cap ø = ø\\
    ii) P(A) \leq 1\\
    A^C = \{x \in \mathcal{S}: x \notin A\}\\
    S = A \cap A^C\\
    A \cup A^C = ø\\
    1 = P(\mathcal{S}) = P(A) + P(A^C)\\
    iii) P(A^C) = 1 - P(A)
\end{align*}

\begin{align*}
    A_n \uparrow A = U_{n=1}^\infty A \to P(A_n) \uparrow P(A)\\
    A_n \subset A_{n+1}\\
    B_1 = A_1\\
    B_k = A_k \setminus A_{k-1}
\end{align*}

\newpage

\begin{align*}
    U_{n=1}^\infty A_i = \{x \in \mathcal{S} : \exists i: x \in A_i\}
\end{align*}

\begin{align*}
    P: \mathcal{S} \to [0, 1]\\
    A, B \in \mathcal{S}\\
    \text{then:}\\
    i)\\
     P(A \cap A^C) = P(B) - P(A \cap B)\\
    B = (B \cap A) \cup (B \cap A^C) \quad\text{B and A complement have no commen elements}\\
    P(B) = P(B\cap A) + P(B \cap A^C)\\
    \\
    ii)\\
     P(A \cup B) = P(A) + P(B) - P(A \cap B)\\
    A = \{1, 2, 3\} \quad B = \{3, 4, 5\}\\
    \\
    A \cup B = A \cup (B \cap A^C)\\
    P(A \cup B) = P(A) + P(B \cap A^C) \quad = P(A) + P(B) - P(A \cap B)\\
    \\
    iii)\\
    A \subset B \to P(A) \leq P(B)\\
    P(B) = P(A) + P(B \cap A^C)\\
    \\
    iv)\\
    (A_n)\\
    A \subset U_{n=1}^\infty A_n\\
    P(A) \leq \sum_{i = 1}^{\infty}P(A_n)\\
    A_n' = A_n \cap A\\
    B_n = A_n' \setminus U_{m=1}^{n-1} A_m'\\
    U_{n=1}^\infty B_n = A \to P(A) = \sum_{n = 1}^{\infty}P(B_n) \leq \sum_{n = 1}^{\infty} P(A_n)
\end{align*}

\newpage

\begin{align*}
    X : S \to \mathbb{R} \quad\text{(R is the sample space)}\\
    \mathcal{X} \subset \mathbb{R}\\
    \mathcal{B}\quad\text{(collection of subsets)}\\
    B \in \mathcal{B}\\
    P_x(B) = P(\{s \in S: X(s)\in B\}) \to P(X^{-1}(B)) \to P(X \in B) = P(\{x \in B\})\\
    \\
    S = \{(1, 1), (1, 2), …, (6, 5), (6, 6)\}\quad\text{(throwing two dice)}\\
    \mathcal{S} = \text{all subsets of $S$}\\
    P(\{(i, j)\}) = \frac{1}{36}\\
    \\
    A = \{(1, 1), …, (6, 6)\} \to A_1 = \{1, 1\}, …, A_6 = \{6, 6\}\\
    U_{i = 1}^6 A_i = A\\
    P(A) = P(U_{i = 1}^6 A_i) = \sum_{i = 1}^{6} P(A_i)= \sum_{i = 1}^{6} \frac{i}{36} = \frac{6}{36} = \frac{1}{6}
\end{align*}

\begin{align*}
    X : S \to \mathbb{R}\\
    X\Big((i, j)\Big) = i + j\\
    \mathcal{X} = \{\text{all the possible sums you can run through}\} \to \mathcal{X} = \{\text{(2, 3, 4, …, 12)}\}\\
    \mathcal{B} = \text{subset of } \mathcal{X}\\
    P_x(\{3\}) = P\Big(X^{-1}(\{3\})\Big) = P\Big((i, j) \in \mathcal{S}: X((i, j)) \in \{3\}\Big)\\
\end{align*}

\begin{align*}
    \mathcal{X} \supset A = \{2, 4, 6, 8, 10, 12\}\\
    P_x(A) = \\
    P(\text{both even}) &= P(\text{first even}) \cdot P(\text{second even}) = \frac{3}{6} \cdot \frac{3}{6} = \frac{9}{36}\\
    P(\text{both odd}) &= P(\text{first odd}) \cdot P(\text{second odd}) = \frac{3}{6} \cdot \frac{3}{6} = \frac{9}{36}\\
    P_x(A) &= \frac{9}{36} + \frac{9}{36} = \frac{18}{36} = \frac{1}{2}
\end{align*}

\newpage

\section{CDF}

Cumulative Distribution Function

\begin{align*}
    P_x\\
    F : \mathbb{R} \to [0, 1] \quad\text{ cdf of } X_1\\
    F(x) = P(X \leq x) = P(s \in S : X(s) \leq x) = P_x\big((-\infty, x]\big)\\
    \big(P_x(A) : A \in B\big)
\end{align*}


\textbf{Properties of CDF:}
if $F$ is the cdf of a random variable $X$, then: 
\begin{align*}
    i) \lim_{x \to -\infty} F(x) = 0 \text{ and }\lim_{x \to \infty} F(x) = 1\\
    \\
    ii) F(x) \leq F(y) \text{ whenever } y \geq x\\
    \\
    iii) \lim_{x \downarrow z} F(x) = F(z)
\end{align*}

\begin{align*}
    x_n \to -\infty \\
    F(x_n) = P_x\big((-\infty, x_n])\big) \downarrow P_x(ø) \quad\text{(converges downwards)}\\
    (-\infty, x_n] \downarrow ø\\
    \\
    (-\infty, x_n] \uparrow (-\infty, \infty) = \mathbb{R}\\
    \\
    ii) (-\infty, x] \subset (-\infty, y] \quad\text{ if } y \geq x\\
    F(x) = P_x\big((-\infty, x]\big) \leq P_x\big((-\infty, y]\big) = F(y)\\
    \\
    z_n \downarrow x \to (-\infty, z_n] \downarrow (-\infty, x] \to F(z_n) \to F(x)\\
    \\
    z_n \uparrow x \to (-\infty, z_n] \uparrow (-\infty, x)
\end{align*}

\textbf{Identically distributed:}

\begin{align*}
    \text{let $x$ and $y$ be a random variable}\\
    \forall B \in \mathcal{B}\\
    P(x \in B) = P(y \in B)\\
    P_x(B) = P_y(B)\\
    \text{This is like flipping two coins, coin one is $x$ and coin two is $y$ (they are not the same coin, hence the same probability)}
\end{align*}

random variables can also be Identically distributed:

\begin{align*}
    \text{if } F_x = F_y \quad \forall x \in \mathbb{R}
\end{align*}

\newpage

\begin{align*}
    F_x(x) = P\big(x \in (-\infty, x]\big) = P_x\big((-\infty, x]\big)\\
    = P_y\big((-\infty, x]\big) \\
    = P\big(y \in (-\infty, x]\big)\\
    = F_y(x)
\end{align*}

\subsection{Continuous and discrete distributions: }

\textbf{$x$ is discrete: }

$x$ has a pmf: $f(x) = P(X=x), \quad x\in\mathbb{R}$

\begin{align*}
    B \in \mathcal{B}\\
    P_x(B) = P(x \in B) = \sum_{t \in B} f(t)\\
    P(\text{even}) = f(2) + f(4) + f(6) \quad\text{(like the example about the dice)}
\end{align*}

\begin{align*}
    F(x) = P_x\big((-\infty, x]\big)\\
    = \sum_{t \leq x} f(t) \quad\text{(\textbf{countable} points that are more than zero)}
\end{align*}

\newpage

\section{Three common  distributions: }

1. \textbf{Bernoulli:} 

\begin{align*}
    X \sim \text{Bernoulli}(p), \quad p \in [0, 1]
\end{align*}

\[
    X = 
    \begin{cases}
        1 & \text{if prob. }p\\
        0 & \text{if prob. } 1-p
    \end{cases}
\]

\[
    f(x)
    \begin{cases}
        p & x = 1\\
        p & x 0 1\\
        p & x \notin \{0, 1\}
    \end{cases}
\]

Ex: we have two different websites and we want to measure website performance of how many people bought stuff on each
of the websites $\to$ \textbf{proportions}
\hfill

2. \textbf{Binomial:}

\begin{align*}
    X \sim \text{Binom}(n, p)\\
    f(x) = \binom{n}{x} p^x (1-p)^{n-x}, \quad x = 0, 1, …, n\\
\end{align*}

3. \textbf{Poisson:}

\begin{align*}
    X \sim \text{Poisson}(\lambda)\\
    f(x) = \frac{\exp (-\lambda)\lambda^x}{x!}, \quad x = 0, 1, 2, … 
\end{align*}

Example: ordered packages, how likely is it that 6 of my packages are going to arrive before lunch? lambda is package arrival intensity

\newpage

\subsection{Continuous random variables}

X is continuous 

\begin{align*}
    f. \\
    f(x) \neq P(X=x)\\
    P(X=x) = 0 \quad\forall x \in \mathbb{R} \quad\text{(The probability of hitting any specific exact value is essentially zero)}
\end{align*}

\begin{align*}
    P(X \leq x) = F(x) = \int_{-\infty}^{x}f(t)dt\\
    P(x \in B) = \int_{-B}f(t)dt = \int_{-\infty}^{\infty}\mathbb{1}_B(t)f(t)dt
\end{align*}

\textbf{Uniform distribution}

$X \sim \text{uniform}(a, b)$
\[
    f(x) = 
    \begin{cases}
        \frac{1}{b -a} & x \in [a, b]\\
        0 & \text{otherwise}
    \end{cases}
\]

\textbf{Normal distribution:}

$X \sim \text{Normal}(\mu, \sigma^2)$

\begin{align*}
    f(x) = \frac{1}{\sqrt{2\pi \sigma^2}}\exp \bigg(\frac{-(x - \mu^2)}{2\sigma^2}\bigg)
\end{align*}

\textbf{Student's distribution}

$X \sim t(v)$

\begin{align*}
    f(x) = \frac{\Gamma\big((v + 1) / 2\big)}{\Gamma(v/2)\sqrt{v\pi}} \bigg(1 + \frac{x^2}{v}\bigg)^{-(v\pi) / 2}
\end{align*}

\textbf{Chi-squared - related to the N dist}

$X \sim \chi^2(p)$

Ugly formula – google it

\newpage

$f$ is a pdf/pmf of r.v. $x$

\begin{align*}
    i) f(x) \geq 0\\
    ii) \int_{-\infty}^{\infty}f(x)dx = 1; \quad \sum_{x \in \mathcal{X}}f(x) = 0
\end{align*}


i) continuous 

\begin{align*}
    \text{as } F(x) \uparrow\quad F(x) = \int_{-\infty}^{x}f(t)dt
\end{align*}

ii) discrete

\begin{align*}
    \sum_{x \in \mathcal{X}} f(x) = \lim_{t \to \infty} \sum_{x \leq t, x \in \mathcal{X}}f(x)\\
    = \lim_{t \to \infty} F(t)\\
    = 1
\end{align*}

\begin{align*}
    \int_{-\infty}^{\infty}f(x) dx = \lim_{t \to \infty} \int_{-\infty}^{t} f(x) dx = \lim_{t \to \infty} F(t) = 1
\end{align*}

\newpage

\section{Creating r.vs from an r.v.}

$X: S \to \mathbb{R}$

$g: \mathbb{R} \to \mathbb{R}$

$Y = g(X) : S \to \mathbb{R}$

$P(y \in A) = P(g(x) \in A) = P\big(\{x \in \mathbb{R}: g(x) \in A\}\big) = P\big(x \in g^{-1}(A)\big)$

If this transformation is well ´-behaved, we can compute the pdf of g(x)

i) If $x$ is discrete, then $Y = g(x)$ is also discrete: $\quad f_y(y) = \sum_{x \in g^{-1}(\{y\})} f_x(x)$

ii) If $x$ is cont., then the output will be discrete: 

$F_Y(y) = \int_{\{x: g(x) \leq y\}}f_X(x)dx = P(g(x) \leq y) = P\Big(x \in g^{-1}\big(\{y\}\big)\Big) = P(Y \leq y)$

iii) if $g$ is strictly monotone increasing, then $h = g^{-1}$ is differentiable, THEN 

$$
F_Y(y) = F_X\big(h(y)\big), \quad f_Y(y) = f_X\big(h(x)\big)h'(y)
$$

$$
F_X(y) = \int_{-\infty}^{g^{-1}(y)}f_X(x)dx = F_X\big(h(y)\big)
$$

\newpage

\section{Mean variance of an arbitrary distribution}

\begin{align*}
    \mathbb{E}g(x) = \int_{-\infty}^{\infty}g(x)f(x)dx\\
    = \sum_{x \in \mathcal{X}}g(x)f(x) \quad\text{(in the discrete case)}\\
    \mathbb{E}X = \int_{-\infty}^{\infty}x(fx)dx\quad\text{{this technically does not exist, hope this helps}}
\end{align*}

we can always write 
\begin{align*}
    g(x) = g^+(x) - g^-(x)\quad\text{(both funcs are positive)}
\end{align*}

expectation exists if: 
\begin{align*}
    \mathbb{E} |g(x)| < \infty
\end{align*}

in the case that 
\begin{align*}
    \mathbb{E} |g(x)| \geq \infty
\end{align*}

we can say that it \textit{may} not exist 

\begin{align*}
    i) \mathbb{E}[\alpha g_1(x)] = \alpha\mathbb{E}[g(x)]\\
    \mathbb{E}[g_1(x) + g_2(x)] = \mathbb{E}[g_1(x)] + \mathbb{E}[g_2(x)]\\
    \\
    ii) g_1(x) \geq 0 \quad\forall f(x) > 0 \to \mathbb{E}g_1(x) \geq 0 \quad\text{(proof below)}
\end{align*}

± inf are implied

\begin{align*}
    \mathbb{E}\alpha g_1(x) = \int \alpha g_1(x)f(x) dx = \alpha \int g(x) f(x) dx = \alpha \mathbb{E}g_1(x)\\
    \mathbb{E} \big(g_1(x) + g_2(x)\big) = \int \big(g_1(x) + g_2(x)\big)f(x)dx = \int g_1(x)f(x) + g_2(x)f(x) dx\\
    = \int g_1(x)f(x)dx + \int g_2(x)f(x)dx
\end{align*}

remember to look at ii) from the notes

\end{document}